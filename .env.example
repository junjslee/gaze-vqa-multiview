# Copy this file to .env and fill your keys.
# The pipeline auto-loads .env from gaze_vqa/.env (or current working directory).

# Select provider for VLM calls: qwen | openai | gemini
VLM_PROVIDER=qwen

# Optional global model override for selected provider.
# If empty, provider defaults are used.
# VLM_MODEL=

# Qwen local model id (used when VLM_PROVIDER=qwen)
QWEN_MODEL_ID=Qwen/Qwen2.5-VL-7B-Instruct

# OpenAI (ChatGPT API)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o
OPENAI_BASE_URL=https://api.openai.com/v1

# Gemini
GEMINI_API_KEY=
GEMINI_MODEL=gemini-2.5-flash
GEMINI_API_BASE=https://generativelanguage.googleapis.com
# Use 0 for short-label pipelines (prevents thought-only MAX_TOKENS empty outputs).
GEMINI_THINKING_BUDGET=0

# Optional timeout (seconds) for API calls
VLM_TIMEOUT_S=90
# Retry controls for transient API/network failures (bounded, not infinite)
VLM_API_MAX_ATTEMPTS=3
VLM_API_RETRY_BACKOFF_S=1.5

# Optional pricing overrides for usage-cost report (USD per 1M tokens)
OPENAI_PRICE_INPUT_PER_1M=2.5
OPENAI_PRICE_CACHED_INPUT_PER_1M=1.25
OPENAI_PRICE_OUTPUT_PER_1M=10.0
GEMINI_PRICE_INPUT_PER_1M=0.30
GEMINI_PRICE_OUTPUT_PER_1M=2.50
